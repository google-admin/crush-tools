#概要 对用CRUSH进行数据处理的一个简单介绍

= CRUSH 指南 =

在这里我们会演示一些CRUSH 工具包的数据处理能力.  首先我们看一看如何处理 Apache 网络服务器的访问日志, 因为这样的数据一般是可以得到的.


== 读取访问日志 ==

要在一个处理工作中使用CRUSH，往往要写少量的自定义代码.  在这里, 为了处理 Apache 访问日志，写一些代码是几乎是必不可少的，因为这些日志文件的默认格式并非真正的字符分隔格式. 一个简单的 Perl 脚本会把这些数据转变成 CRUSH 更容易处理的用字符分隔的格式.

对于下面这个服务器配置：

{{{
LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %b" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent
}}}

可以写个脚本做到, 我们姑且叫它 `aal2d` ("Apache access log to delimited"):

{{{
#!/usr/bin/perl -w
use strict;
my $output_delim = $ENV{DELIMITER} || chr(0xfe);

# (dropping the RFC 1413 identity field)
print join($output_delim,
           qw(IP Auth-User Time Request Response Size Referrer User-Agent)),
      qq(\n);

while (<>) {
  /([\d\.]+) - ([^ ]+) \[([^\]]+)\] "([^"]+)" (\d+) (\d+|-) "([^"]+)" "([^"]+)"/;
  print join($output_delim,
             ($1, $2, $3, $4, $5,
              $6 eq '-' ? 0 : $6,
              $7 eq '-' ? '' : $7,
              $8)),
        qq(\n);
}

exit(0);
}}}

在这里使用缺省的分隔字符 'þ'，是因为它跟真实数据发生冲突的概率很小.  既然解决了分隔字符的问题, 我们就可以用这些数据做些有趣的事儿.

在下面的例子当中, 使用相对于 "access_log" 的相对路径.  要对你计算机中的 Apache 日志进行操作, `cd` 到 `/var/log/httpd`, 或者 [http://crush-tools.googlecode.com/files/access_log.tar.gz 下载示例数据] 然后在这些数据所在的文件夹运行命令.

== 简单的聚集 ==

比如说，我们现在想知道每一个登陆我们网站的用户点击了多少下.  `aggregate` (AggregateUserDocs) 就是为此设计的.

{{{
aal2d access_log | aggregate -p -k 2 -c 1

# Or for CRUSH release 2009-01 or greater:

aal2d access_log | aggregate -K Auth-User -C IP
}}}

详细用法请参考 AggregateUserDocs , 但在这儿我们做聚集仅仅是把 字段  2 当做主键(key) 然后对字段 1计数.

输出大致是这样的：

{{{
Auth-UserþIP
-þ1127
bvanhornþ103
hoofoosþ303
hotshotþ3
shadrackþ13
snimþ12
sunnyjimþ141
}}}

如果我们想按日把数据分开, 我们可以使用 `convdate` (ConvdateUserDocs) 转换日期格式，然后只包含日期，把日期当做 key field.

{{{
aal2d access_log |
  convdate -f 3 -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  aggregate -p -k 3,2 -c 1

# Or for CRUSH release 2009-01 or greater:

aal2d access_log |
  convdate -F Time -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  aggregate -K Time,Auth-User -C IP
}}}

函数 `convdate` 的调用使用"08/Jul/2008:04:27:26 -0600" 这样的字符串作为参数，然后把字符串转变成 "2008-07-08" 的格式输出

{{{
TimeþAuth-UserþIP
2008-07-06þ-þ28
2008-07-06þhoofoosþ78
2008-07-06þsunnyjimþ17
2008-07-07þ-þ569
2008-07-07þhoofoosþ93
2008-07-07þshadrackþ6
2008-07-07þsnimþ11
2008-07-08þ-þ188
2008-07-08þbvanhornþ59
2008-07-08þhoofoosþ21
2008-07-08þshadrackþ7
2008-07-08þsunnyjimþ45
2008-07-09þ-þ342
2008-07-09þbvanhornþ44
2008-07-09þhoofoosþ111
2008-07-09þhotshotþ3
2008-07-09þsnimþ1
2008-07-09þsunnyjimþ79
}}}


== 更多的格式 ==

在按日统计点击率的例子中, 如果用户较少，可以用每一列表示一个用户，每一行表示一个日期. `pivot` (PivotUserDocs) 工具可以做到这点：

{{{
aal2d access_log |
  convdate -f 3 -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  aggregate -p -k 3,2 -c 1 |
  pivot -k -f 1 -p 2 -v 3

# Or for CRUSH release 2009-01 or greater:

aal2d access_log |
  convdate -F Time -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  aggregate -K Time,Auth-User -C IP
  pivot -F Time -P Auth-User -A IP
}}}

我们得到这样的输出：

{{{
Timeþ-: IPþbvanhorn: IPþhoofoos: IPþhotshot: IPþshadrack: IPþsnim: IPþsunnyjim: IP
2008-07-06þ28þ0þ78þ0þ0þ0þ17
2008-07-07þ569þ0þ93þ0þ6þ11þ0
2008-07-08þ188þ59þ21þ0þ7þ0þ45
2008-07-09þ342þ44þ111þ3þ0þ1þ79
}}}

将命令管道连接到 `csvformat` (CsvformatUserDocs) 会产生适于在电子表格程序中查看的的逗号分隔的文件.  我们可以用 `translate_field` (Translate_fieldUserDocs) 改变所有没有登录的会话名为 "匿名".  要把聚集结果中的"IP" 头转变成有意义的东西, 可以用标准的 `sed(1)` 工具 处理标题:

{{{
# CRUSH 2008-10 and earlier:
aal2d access_log |
  (read header;
   echo "$header" | sed -e 's/IP/Page-Views/';
   cat -) |
  convdate -f 3 -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  translate_field -f 2 -m '-=Anonymous' |
  aggregate -p -k 3,2 -c 1 |
  pivot -k -f 1 -p 2 -v 3 |
  csvformat 
}}}

或者加一个 `-l` 选项到聚集函数的调用中:

{{{
# CRUSH 2009-01 and later:
aal2d access_log |
  convdate -F Time -i "%d/%b/%Y:%H:%M:%S %z" -o "%Y-%m-%d" |
  translate_field -F Auth-User -m '-=Anonymous' |
  aggregate -K Time,Auth-User -C IP -l Page-Views|
  pivot -k -F Time -P Auth-User -A Page-Views |
  csvformat 
}}}


输出:

{{{
"Time","Anonymous: Page-Views","bvanhorn: Page-Views","hoofoos: Page-Views","hotshot: Page-Views","shadrack: Page-Views","snim: Page-Views","sunnyjim: Page-Views"
"2008-07-06","28","0","78","0","0","0","17"
"2008-07-07","569","0","93","0","6","11","0"
"2008-07-08","188","59","21","0","7","0","45"
"2008-07-09","342","44","111","3","0","1","79"
}}}


== 连接数据集 ==

很多时候要把数据集连接起来.  一个方法就是使用 `mergekeys`(MergekeysUserDocs)工具, 它使用列标题连接不同的数据集.  比如说有一个文件 `food.log` 包含

{{{
Auth-UserþFavorite-Food
bvanhornþEggs
hoofoosþHam
hotshotþFish
shadrackþCake
snimþSoup
sunnyjimþFrankfurters
}}}

现在要把这个文件跟上面的第一个聚集结果连接 ，可以这样：

{{{
aal2d access_log | aggregate -p -k 2 -c 1 > page_views.log
# or...
aal2d access_log | aggregate -K Auth-User -C IP > page_views.log
# and then:
mergekeys page_views.log food.log | csvformat
}}}

就会得到

{{{
"Auth-User","IP","Favorite-Food"
"-","1127",""
"bvanhorn","103","Eggs"
"hoofoos","303","Ham"
"hotshot","3","Fish"
"shadrack","13","Cake"
"snim","12","Soup"
"sunnyjim","141","Frankfurters"
}}}

注意 `mergekeys` 要求：
 * 所有的 key fields 要有相同的列标题
 * 所有的 key fields 在两个文件中都要在每一行的开始
 * 两个文件都已经按 key fields 排好序


== 所以... ==

通过建立 CRUSH 的管道和标准的Unix 工具, 复杂的转换和格式化都可以使用简单的 shell 脚本来完成.